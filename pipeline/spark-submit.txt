For many PySpark applications, the following template will suffice to launch a job: 
We invoke “spark-submit”, and pass it the optional master argument, which is either a URL specific to the cluster manager or a string like “local[*]”. 
This tells Spark where it can get resources from, which is either a real cluster or simply your local machine. 
The latter is often good enough for tests. 
If our PySpark data pipeline relies on more than just one Python module, we will need to copy these modules to all nodes, 
so that each Python interpreter on the cluster workers knows where to find the details of the functions they must execute. 
We typically provide the modules that aren’t already installed on the worker nodes through zip files, 
and that is what the “--py-files” argument does for us: it copies these dependencies over to the workers. 
Finally, we tell Spark which file is the main file, the entrypoint to our application. 
It is the file that contains code to trigger the creation of the SparkSession. 
If that file, that Python module actually, parses command line arguments, through the “argparse” module for example, 
then we may supply these extra arguments at the end of the “spark-submit” command.

spark-submit \	On your path, if Spark is installed
--master "local[*]" \	URL of the cluster manager
--py-file PY_FILES \	Comma-separated list of zip, egg or py
MAIN_PYTHON_FILE \	Path to the module to be run

The “--py-files” option can take a comma separated list of files that will be added on each worker’s PYTHONPATH, 
which lists the places where the Python interpreter will look for modules. 
Zip files are a common way to package and distribute your code. 
To create a proper zip-file, ready to be used with PySpark, navigate in the shell to the directory that contains the root folder of your module. 
In the example shown, that would be the folder containing the “pydiaper” folder. 
Once there, invoke the “zip” utility, enabling it to recursively add all files in all subfolders, by passing the “--recurse-paths” flag. 
Provide a name for the resulting compressed archive and finally provide the name of the folder you want to compress. 
The resulting zip file can be passed as is to “spark-submit”’s “--py-files” argument. 
Both “clean_prices.py” and “clean_ratings.py” are Spark jobs, as they start a SparkSession. So either one could be given as the main Python file.

zip --recurse-paths dependencies.zip pydiaper
spark-submit --py-files dependencies.zip pydiaper/cleaning/clean_prices.py